---
title: "Practical Machine Learning Project"
author: "Davide Tiana"
date: "23 settembre 2015"
output: html_document
---

### Synopsis

The goal of this analysis is to build a classification model able to distinguish if a barbell lift is done in correct way or in one of 5 different wrong ways. The model has to classify the lifts by means of physical measures, i.e. linear and angular accelerations, from a set of fitbands fastened on bodies of 6 different subjects. The dataset used to build the model has been initially cleaned by meaningless covariates. Then two models have been built on a training subset and one has been choosen as the most performant, via assessment on a validation subset. Lastly the winning model has been tested agaist a test dataset already scored.


### Data cleaning and covariates transformations

The dataset has 158 covariates, a factor target with 5 levels and an ID counter in the first column
There are 36 factor covariates, 2 date/time variables and the other 120 covariates are numeric.
The following table shows a sample of the covariate's statistics
```{r, echo=FALSE, cache=TRUE}
## loading libraries and setting global options##

library(caret); library(moments)
options(digits = 3, scipen = 7)

## loading data from locally(working directory) saved csv ##

test = read.csv("pml-testing.csv")
dataraw = read.csv("pml-training.csv")

## building variables statistics descriptive table ##

descriptive= data.frame(name=1:159, type=1:159, m_e_a_n=1:159, std_dev=1:159, m_issing=1:159, skewness=1:159, kurtosis=1:159)
descriptive[1,1]=names(dataraw)[1]; descriptive[1,2]="ID"; descriptive[1,3:7]=NA
for(i in 2:159){
                descriptive[i,1]= names(dataraw)[i]
                descriptive[i,2]=class(dataraw[1,i])
                descriptive[i,3]=sum(is.na(dataraw[,i]))
                descriptive[i,4]=sum(is.null(dataraw[,i]))
                descriptive[i,5] = round((as.numeric(descriptive[i,3]) + as.numeric(descriptive[i,4]))/19622,3)
                if(!descriptive[i,2]=="factor"){
                                                descriptive[i,3]=round(mean(dataraw[,i]), 3)
                                                descriptive[i,4]=round(sd(dataraw[,i]), 3)
                                                descriptive[i,6]=round(skewness(dataraw[,i], na.rm=T), 3)
                                                descriptive[i,7]=round(kurtosis(dataraw[,i], na.rm=T), 3)                
                                                }
                else{   descriptive[i,3]="NA"
                        descriptive[i,4]="NA"
                        descriptive[i,6]="NA"
                        descriptive[i,3]="NA"
                        }
                }

rm(i)

print(descriptive[c(1:7,18),])




```
There is a large amount of missing values in the dataset, that is  67 variables have more than 97% of missing values (last row in the table) then We have choosen to drop all these variables.We have choosen even to drop not meaningful covariates like the ID, the "user_name" and the time/date related covariates. Lastly searching and dropping near zero variables.
```{r, echo=FALSE,cache=TRUE}

## dropping ID, user_name and date/time covariates ##

tidy1 = dataraw[,6:160]
mytest1 = test[,6:160]

## subsetting raw data dropping variables with too many missing values and the ID ##

keep =c(descriptive$m_issing[6:159]<0.97, T)
tidy =tidy1[,keep]
mytest=mytest1[,keep]
rm(tidy1, mytest1)

## dropping near zero variables ##

nzv <- nearZeroVar(tidy)
tidy = tidy[, -nzv]
mytest=mytest[,-nzv]

```
The final tidy dataset results in 53 numeric or integer covariates.
Skewness and kurtosis are not so large so We have choosen to keep variables as they are without applying transformation like logaritmic to smooth the distributions toward a more normal-like one, beside some of the models We will use like trees dont need such kind of preprocessing.

### Models building

First our training strategy, We have 19622 observations available so there are enough to avoid cross-validation.
We have choosen to subset the data in 75% for training and 25% for validation, then We will build 
2 different models on the training set:

- a Random Forest (RF: rf/caret), preferred to trees because already encapsulate trees;

- a Tree with CART splitting rules (CART: rpart/caret), for reference toward the more complex Random Forest.
```{r, echo=FALSE, cache=TRUE}

library(mboost); library(randomForest)

## partitioning training & validation ##

inTrain <- createDataPartition(y=tidy$classe, p=0.75, list=F)
training <- tidy[inTrain,]
validation <- tidy[-inTrain,]

## building models ##

RFmodelFit <- train(classe~., data = training, method="rf")

CARTmodelFit <- train(classe~., data = training, method="rpart")

## building confusion matrix on validation set ##

RFval <- predict(RFmodelFit, newdata = validation)
RFmatrix <- confusionMatrix(validation$classe, RFval)

CARTval <- predict(CARTmodelFit, newdata = validation)
CARTmatrix <- confusionMatrix(validation$classe, CARTval)

```

### Model Assessment

To choose which model using for our classification on the 20 test cases part of the Coursera assignement, We build the  confusion matrices for the two models on the validation set of data.
The statistical measures of classification performance for the Random Forest are show in the following table
```{r, echo=FALSE}

print(RFmatrix)

```

For CART tree instead the confusion matrix and statistical measures of performance are as follow
```{r, echo=FALSE}

print(CARTmatrix)

```

As show by the two tables, Random Forest has better performance so We will use the Random Forest model as our classification model

NOTE EXPECTED OUT of SAMPLE ERROR: from the 99.8% value of accuracy, We can estimate the out of sample error in 0.2%

### Classifying new cases 

Lastly We apply the selected model to the test dataset to test the goodness of the model against the Coursera predicted classes.
```{r}

## classifying test set ##

predicted_test <- predict(RFmodelFit, newdata=mytest)

## convert predictions to character vector ##

predicted_test <- as.character(predicted_test)

## create function to write predictions to files ##

pml_write_files <- function(x) {
    n <- length(x)
    for(i in 1:n) {
                        filename <- paste0("problem_id_", i, ".txt")
                        write.table(x[i], file=filename, quote=F, row.names=F, col.names=F)
                   }
                                }

## create prediction files to submit ##

pml_write_files(predicted_test)

```

